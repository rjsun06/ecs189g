{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17d0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bffdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d9afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class method:\n",
    "    '''\n",
    "    MethodModule: Abstract Class\n",
    "    Entries: method_name: the name of the MethodModule \n",
    "             method_description: the textual description of the MethodModule\n",
    "             \n",
    "             method_start_time: start running time of MethodModule\n",
    "             method_stop_time: stop running time of MethodModule\n",
    "             method_running_time: total running time of the MethodModule\n",
    "             method_training_time: time cost of the training phrase\n",
    "             method_testing_time: time cost of the testing phrase\n",
    "    '''\n",
    "    \n",
    "    method_name = None\n",
    "    method_description = None\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    method_start_time = None\n",
    "    method_stop_time = None\n",
    "    method_running_time = None\n",
    "    method_training_time = None\n",
    "    method_testing_time = None\n",
    "\n",
    "    # initialization function\n",
    "    def __init__(self, mName=None, mDescription=None):\n",
    "        self.method_name = mName\n",
    "        self.method_description = mDescription\n",
    "\n",
    "    # running function\n",
    "    @abc.abstractmethod\n",
    "    def run(self, trainData, trainLabel, testData):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c8736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be1d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328feded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class dataset:\n",
    "    \"\"\" \n",
    "    dataset: Abstract Class \n",
    "    Entries: dataset_name: the name of the dataset\n",
    "             dataset_description: the textual description of the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_name = None\n",
    "    dataset_descrition = None\n",
    "    \n",
    "    dataset_source_folder_path = None\n",
    "    dataset_source_file_name = None\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    # initialization function\n",
    "    def __init__(self, dName=None, dDescription=None):\n",
    "        '''\n",
    "        Parameters: dataset name: dName, dataset description: dDescription\n",
    "        Assign the parameters to the entries of the base class\n",
    "        '''\n",
    "        self.dataset_name = dName\n",
    "        self.dataset_descrition = dDescription\n",
    "    \n",
    "    # information print function\n",
    "    def print_dataset_information(self):\n",
    "        '''\n",
    "        Print the basic information about the dataset class\n",
    "        inclduing the dataset name, and dataset description\n",
    "        '''\n",
    "        print('Dataset Name: ' + self.dataset_name)\n",
    "        print('Dataset Description: ' + self.dataset_descrition)\n",
    "\n",
    "    # dataset load abstract function\n",
    "    @abc.abstractmethod\n",
    "    def load(self):\n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ffcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class evaluate:\n",
    "    \"\"\" \n",
    "    evaluate: Abstract Class\n",
    "    Entries: \n",
    "    \"\"\"\n",
    "    \n",
    "    evaluate_name = None\n",
    "    evaluate_description = None\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    # initialization function\n",
    "    def __init__(self, eName=None, eDescription=None):\n",
    "        self.evaluate_name = eName\n",
    "        self.evaluate_description = eDescription\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def evaluate(self):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe46341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class result:\n",
    "    \"\"\"\n",
    "    ResultModule: Abstract Class\n",
    "    Entries: \n",
    "    \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    \n",
    "    result_name = None\n",
    "    result_description = None\n",
    "    \n",
    "    result_destination_folder_path = None\n",
    "    result_destination_file_name = None\n",
    "    \n",
    "    # initialization function\n",
    "    def __init__(self, rName=None, rType=None):\n",
    "        self.result_name = rName\n",
    "        self.result_description = rType\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def save(self):\n",
    "        return\n",
    " \n",
    "    @abc.abstractmethod\n",
    "    def load(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157f98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class setting:\n",
    "    '''\n",
    "    SettingModule: Abstract Class\n",
    "    Entries: \n",
    "    '''\n",
    "    \n",
    "    setting_name = None\n",
    "    setting_description = None\n",
    "    \n",
    "    dataset = None\n",
    "    method = None\n",
    "    result = None\n",
    "    evaluate = None\n",
    "\n",
    "    def __init__(self, sName=None, sDescription=None):\n",
    "        self.setting_name = sName\n",
    "        self.setting_description = sDescription\n",
    "    \n",
    "    def prepare(self, sDataset, sMethod, sResult, sEvaluate):\n",
    "        self.dataset = sDataset\n",
    "        self.method = sMethod\n",
    "        self.result = sResult\n",
    "        self.evaluate = sEvaluate\n",
    "\n",
    "    def print_setup_summary(self):\n",
    "        print('dataset:', self.dataset.dataset_name, ', method:', self.method.method_name,\n",
    "              ', setting:', self.setting_name, ', result:', self.result.result_name, ', evaluation:', self.evaluate.evaluate_name)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load_run_save_evaluate(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18ca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83beee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "class Dataset_Loader(dataset):\n",
    "    data = None\n",
    "    dataset_source_folder_path = None\n",
    "    dataset_source_file_name = None\n",
    "\n",
    "    def __init__(self, dName=None, dDescription=None):\n",
    "        super().__init__(dName, dDescription)\n",
    "    \n",
    "    def load(self):\n",
    "        print('loading data...')\n",
    "        X = []\n",
    "        y = []\n",
    "        with open(self.dataset_source_folder_path + self.dataset_source_file_name, 'r') as f:\n",
    "            f_csv = csv.reader(f)\n",
    "            for row in f_csv:\n",
    "                elements=[int(i) for i in row]\n",
    "                X.append(elements[1:])\n",
    "                y.append(elements[0])\n",
    "                #print(row[1:4])\n",
    "                #print(row[0])\n",
    "        f.close()\n",
    "        return {'X': X, 'y': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585e4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "\n",
    "class Evaluate_Accuracy(evaluate):\n",
    "    data = None\n",
    "    \n",
    "    def evaluate(self):\n",
    "        #print('evaluating performance...')\n",
    "        return accuracy_score(self.data['true_y'], self.data['pred_y'])\n",
    "\n",
    "    def full_evaluate(self, average):\n",
    "        result=self.data\n",
    "        print('testing',self.evaluate_name)\n",
    "        print(\"overall acc: \", accuracy_score(result['true_y'], result['pred_y']))\n",
    "        print(average, \"pre: \", precision_score(result['true_y'], result['pred_y'], average=average))\n",
    "        print(average, \"recal: \", recall_score(result['true_y'], result['pred_y'], average=average))\n",
    "        print(average, \"f1: \", f1_score(result['true_y'], result['pred_y'], average=average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769d4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Method_CNN(method, nn.Module):\n",
    "    data = None\n",
    "\n",
    "    def __init__(self, mName, mDescription, _max_epoch, _learning_rate, device):\n",
    "        self.device=device\n",
    "        self.max_epoch = _max_epoch\n",
    "        self.learning_rate = _learning_rate\n",
    "        method.__init__(self, mName, mDescription)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.fc_layer_1 = nn.\n",
    "        self.fc_layer_1 = nn.Linear(784, 80).to(self.device)\n",
    "\n",
    "        self.activation_func_1 = nn.CELU()\n",
    "\n",
    "        self.fc_layer_2 = nn.Linear(80, 10).to(self.device)\n",
    "        \n",
    "        self.activation_func_f = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward propagation'''\n",
    "        # hidden layer embeddings\n",
    "        h = self.activation_func_1(self.fc_layer_1(x))\n",
    "        # outout layer result\n",
    "        # self.fc_layer_2(h) will be a nx2 tensor\n",
    "        # n (denotes the input instance number): 0th dimension; 2 (denotes the class number): 1st dimension\n",
    "        # we do softmax along dim=1 to get the normalized classification probability distributions for each instance\n",
    "        y_pred = self.activation_func_f(self.fc_layer_2(h))\n",
    "        return y_pred\n",
    "\n",
    "    # backward error propagation will be implemented by pytorch automatically\n",
    "    # so we don't need to define the error backpropagation function here\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # check here for the torch.optim doc: https://pytorch.org/docs/stable/optim.html\n",
    "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        # check here for the gradient init doc: https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n",
    "\n",
    "        # check here for the nn.CrossEntropyLoss doc: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        # for training accuracy investigation purpose\n",
    "        accuracy_evaluator = Evaluate_Accuracy('training evaluator', '')\n",
    "\n",
    "        # it will be an iterative gradient updating process\n",
    "        # we don't do mini-batch, we use the whole input as one batch\n",
    "        # you can try to split X and y into smaller-sized batches by yourself\n",
    "        loss_list=[]\n",
    "        x=torch.FloatTensor(np.array(X)).to(self.device)\n",
    "        y_true=torch.LongTensor(np.array(y)).to(self.device)\n",
    "        for epoch in range(self.max_epoch): # you can do an early stop if self.max_epoch is too much...\n",
    "            # get the output, we need to covert X into torch.tensor so pytorch algorithm can operate on it\n",
    "\n",
    "            y_pred = self.forward(x)\n",
    "            # convert y to torch.tensor as well\n",
    "\n",
    "            # calculate the training loss\n",
    "\n",
    "            train_loss = loss_function(y_pred, y_true)\n",
    "\n",
    "            # check here for the loss.backward doc: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "            # do the error backpropagation to calculate the gradients\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            # check here for the opti.step doc: https://pytorch.org/docs/stable/optim.html\n",
    "            # update the variables according to the optimizer and the gradients calculated by the above loss.backward function\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch%100 == 0:\n",
    "                loss_list.append(train_loss.item())\n",
    "                accuracy_evaluator.data = {'true_y': y_true.cpu(), 'pred_y': y_pred.max(1)[1].cpu()}\n",
    "                print('Epoch:', epoch, 'Accuracy:', accuracy_evaluator.evaluate(), 'Loss:', train_loss.item())\n",
    "        return(loss_list)\n",
    "    \n",
    "    def test(self, X):\n",
    "        # do the testing, and result the result\n",
    "        y_pred = self.forward(torch.FloatTensor(np.array(X)).to(self.device))\n",
    "        # convert the probability distributions to the corresponding labels\n",
    "        # instances will get the labels corresponding to the largest probability\n",
    "        return y_pred.max(1)[1]\n",
    "    \n",
    "    def run(self):\n",
    "        print('method running...')\n",
    "        print('--start training...')\n",
    "        loss_list=self.train(self.data['train']['X'], self.data['train']['y'])\n",
    "        print('--start testing...')\n",
    "        pred_y = self.test(self.data['test']['X']).cpu()\n",
    "        print(pred_y)\n",
    "        print(\"pred: \", pred_y, \" true: \", self.data['test']['y'])\n",
    "        return {'result':{'pred_y': pred_y, 'true_y': self.data['test']['y']},'loss':loss_list}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8581ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Result_Saver(result):\n",
    "    data = None\n",
    "    fold_count = None\n",
    "    result_destination_folder_path = None\n",
    "    result_destination_file_name = None\n",
    "    \n",
    "    def save(self):\n",
    "        print('saving results...')\n",
    "        f = open(self.result_destination_folder_path + self.result_destination_file_name + '_' + str(self.fold_count), 'wb')\n",
    "        pickle.dump(self.data, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c296314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "class Setting_KFold_CV(setting):\n",
    "    train   =None\n",
    "    test    =None\n",
    "    method  =None\n",
    "    result  =None\n",
    "    evaluate=None\n",
    "    test_data = None\n",
    "\n",
    "    def load_run_save_evaluate(self,stage):\n",
    "        # load dataset\n",
    "        self.train_data = self.train.load()\n",
    "        self.test_data=self.test.load()\n",
    "        train_data = self.train_data\n",
    "        test_data=self.test_data\n",
    "\n",
    "        score_list = []\n",
    "        loss_list=[]\n",
    "        for i in range(stage):\n",
    "            test_index  =   np.random.randint(10000,size=3)\n",
    "           # train_index =   np.random.randint(60000,size=_size)\n",
    "\n",
    "            X_train, X_test = np.array(train_data['X']), np.array(test_data['X'])[test_index]\n",
    "            y_train, y_test = np.array(train_data['y']), np.array(test_data['y'])[test_index]\n",
    "\n",
    "            # run MethodModule\n",
    "            self.method.data = {'train': {'X': X_train, 'y': y_train}, 'test': {'X': X_test, 'y': y_test}}\n",
    "            learned_result = self.method.run()\n",
    "\n",
    "            self.evaluate.data = learned_result['result']\n",
    "            score_list.append(self.evaluate.evaluate())\n",
    "\n",
    "            loss_list+=learned_result['loss']\n",
    "        pl.plot(range(0,len(loss_list)*100,100),loss_list,label='loss', color='purple')\n",
    "        pl.show()\n",
    "        return np.mean(score_list), np.std(score_list)\n",
    "\n",
    "    def do_evaluate(self):\n",
    "        train_data =self.train_data\n",
    "        test_data = self.test_data\n",
    "        #test_index = np.random.randint(10000, size=size)\n",
    "\n",
    "        pred_y = self.method.test(np.array(train_data['X'])).cpu()\n",
    "        true_y = np.array(train_data['y'])\n",
    "        self.evaluate.data = {'pred_y': pred_y, 'true_y': true_y}\n",
    "        self.result.data = self.evaluate.data\n",
    "        self.result.fold_count = 98\n",
    "        self.result.save()\n",
    "\n",
    "        pred_y=self.method.test(np.array(test_data['X'])).cpu()\n",
    "        true_y = np.array(test_data['y'])\n",
    "        self.evaluate.data ={'pred_y': pred_y, 'true_y': true_y}\n",
    "        self.result.data=self.evaluate.data\n",
    "        self.result.fold_count=99\n",
    "        self.result.save()\n",
    "\n",
    "\n",
    "        return self.evaluate.evaluate()\n",
    "\n",
    "    def __init__(self, sName=None, sDescription=None):\n",
    "        self.setting_name = sName\n",
    "        self.setting_description = sDescription\n",
    "\n",
    "    def prepare(self, _train, _test, sMethod, sResult, sEvaluate):\n",
    "        self.train=_train\n",
    "        self.test=_test\n",
    "        self.method = sMethod\n",
    "        self.result = sResult\n",
    "        self.evaluate = sEvaluate\n",
    "\n",
    "    def print_setup_summary(self):\n",
    "        print('trainset:', self.train.dataset_name, 'testset:', self.test.dataset_name, ', method:', self.method.method_name,\n",
    "              ', setting:', self.setting_name, ', result:', self.result.result_name, ', evaluation:',\n",
    "              self.evaluate.evaluate_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef9d8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "************ Start ************\n",
      "trainset: stage2train testset: stage2test , method: multi-layer perceptron , setting: k fold cross validation , result: saver , evaluation: accuracy\n",
      "loading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m setting_obj\u001b[38;5;241m.\u001b[39mprepare(trainset_obj, testset_obj, method_obj, result_obj, evaluate_obj)\n\u001b[0;32m     46\u001b[0m setting_obj\u001b[38;5;241m.\u001b[39mprint_setup_summary()\n\u001b[1;32m---> 47\u001b[0m mean_score, std_score \u001b[38;5;241m=\u001b[39m \u001b[43msetting_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_run_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m************ Overall Performance ************\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP Accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(mean_score) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m +/- \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(std_score))\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mSetting_KFold_CV.load_run_save_evaluate\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_run_save_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m,stage):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     17\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mDataset_Loader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_source_folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_source_file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     16\u001b[0m     f_csv \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m f_csv:\n\u001b[0;32m     18\u001b[0m         elements\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[0;32m     19\u001b[0m         X\u001b[38;5;241m.\u001b[39mappend(elements[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2800.0_x64__qbz5n2kfra8p0\\lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalDecoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '0'\n",
    "#---- Multi-Layer Perceptron script ----\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "if 1:\n",
    "    #---- parameter section -------------------------------\n",
    "    np.random.seed(2)\n",
    "    torch.manual_seed(2)\n",
    "    #------------------------------------------------------\n",
    "\n",
    "    # ---- objection initialization setction ---------------\n",
    "    trainset_obj = Dataset_Loader('stage2train', '')\n",
    "    trainset_obj.dataset_source_folder_path = '../../data/stage_2_data/'\n",
    "    trainset_obj.dataset_source_file_name = 'train.csv'\n",
    "\n",
    "    testset_obj = Dataset_Loader('stage2test', '')\n",
    "    testset_obj.dataset_source_folder_path = '../../data/stage_2_data/'\n",
    "    testset_obj.dataset_source_file_name = 'test.csv'\n",
    "\n",
    "\n",
    "\n",
    "    result_obj = Result_Saver('saver', '')\n",
    "    result_obj.result_destination_folder_path = '../../result/stage_2_result/MLP_'\n",
    "    result_obj.result_destination_file_name = 'prediction_result'\n",
    "\n",
    "    setting_obj = Setting_KFold_CV('k fold cross validation', '')\n",
    "    #setting_obj = Setting_Tra\n",
    "    # in_Test_Split('train test split', '')\n",
    "\n",
    "    evaluate_obj = Evaluate_Accuracy('accuracy', '')\n",
    "    # ------------------------------------------------------\n",
    "    #for learning_rate in [10e-5,10e-6,10e-7]:\n",
    "    #    for epoch in [500,1000,2000]:\n",
    "    #        for size in [100,500,1000]:\n",
    "    learning_rate=10e-5\n",
    "    epoch=8000\n",
    "    stage=1\n",
    "    # ---- running section ---------------------------------\n",
    "    method_obj = Method_CNN('multi-layer perceptron', '', epoch, learning_rate,device).to(device)\n",
    "    print('************ Start ************')\n",
    "    setting_obj.prepare(trainset_obj, testset_obj, method_obj, result_obj, evaluate_obj)\n",
    "    setting_obj.print_setup_summary()\n",
    "    mean_score, std_score = setting_obj.load_run_save_evaluate(stage)\n",
    "    print('************ Overall Performance ************')\n",
    "    print('MLP Accuracy: ' + str(mean_score) + ' +/- ' + str(std_score))\n",
    "    print('************ final evaluation ************')\n",
    "    performance = setting_obj.do_evaluate()\n",
    "    print('final performance: ' + str(performance))\n",
    "    print('************ Finish ************')\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf42dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738b221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
